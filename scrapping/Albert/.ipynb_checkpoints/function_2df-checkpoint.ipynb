{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n"
     ]
    }
   ],
   "source": [
    "# Connect with mongodb\n",
    "\n",
    "try:\n",
    "    with open(\"credentials.txt\", 'r', encoding='utf-8') as f:\n",
    "        [name,password,url,dbname]=f.read().splitlines()\n",
    "        conn=pymongo.MongoClient(\"mongodb://{}:{}@{}/{}\".format(name,password,url,dbname))\n",
    "    print (\"Connected successfully!!!\")\n",
    "except pymongo.errors.ConnectionFailure as e:\n",
    "    print (\"Could not connect to MongoDB: %s\" % e) \n",
    "    \n",
    "\n",
    "    \n",
    "db = conn['tweempact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert Prat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: collection_names is deprecated. Use list_collection_names instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['system.indexes', 'tweets_noRT', 'users_noRT', 'tweets', 'tweet_users']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mongo(collections, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n",
    "    import pandas as pd\n",
    "    import pymongo\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "    # Connect with mongodb\n",
    "\n",
    "    try:\n",
    "        with open(\"credentials.txt\", 'r', encoding='utf-8') as f:\n",
    "            [name,password,url,dbname]=f.read().splitlines()\n",
    "            conn=pymongo.MongoClient(\"mongodb://{}:{}@{}/{}\".format(name,password,url,dbname))\n",
    "        print (\"Connected successfully!!!\")\n",
    "    except pymongo.errors.ConnectionFailure as e:\n",
    "        print (\"Could not connect to MongoDB: %s\" % e) \n",
    "     \n",
    "    db = conn['tweempact']\n",
    "    df1 = pd.DataFrame()\n",
    "    for collection in collections:\n",
    "        cursor = db[collection].find(query)\n",
    "        df =  pd.DataFrame(list(cursor))\n",
    "        if no_id:\n",
    "            del df['_id']\n",
    "        df1 = df1.append(df)\n",
    "    df1 = df1.reset_index(drop = True)\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert Prat\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6201: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "df = read_mongo([\"tweets\", \"tweets_noRT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39283"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    " def author_dataset_conversion(df):   \n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    cleaned = df[[\"id\", \"created_at\",\"entities\", \"extended_entities\",\"favorite_count\",\"retweet_count\",\"source\",\"text\", \"truncated\",\"user\",\"quoted_status_id\"]]\n",
    "    ids = [user[\"id\"] for user in cleaned[\"user\"]]\n",
    "    cleaned[\"id_num\"] = ids\n",
    "\n",
    "    #preparing output variables\n",
    "    output = pd.DataFrame()\n",
    "    users = []\n",
    "    mean_RT_last10,mean_FC_last10 = [],[]\n",
    "    sd_RT,sd_FC  = [],[]\n",
    "    y1,y2=[],[]\n",
    "\n",
    "    for _id in np.unique(ids):\n",
    "        subset = cleaned[cleaned[\"id_num\"]==_id].reset_index(drop=True)\n",
    "\n",
    "        #filtering tweets from all tweets and RT\n",
    "        l = list(subset[\"text\"])\n",
    "        filtr = [\"RT\" not in i for i in l]\n",
    "        subset = subset[pd.Series(filtr)]\n",
    "        if len(subset)>=11:\n",
    "            reindexed = subset[\"created_at\"].reset_index(drop=True)\n",
    "            #convert tweet date into python date\n",
    "            subset[\"created_at_pydate\"] = [time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(reindexed[i],'%a %b %d %H:%M:%S +0000 %Y'))for i in range(0,len(reindexed))]\n",
    "            subset = subset.sort_values(by=\"created_at_pydate\").reset_index(drop=True)\n",
    "\n",
    "            last10_tweets = subset[-11:-1]\n",
    "            last_tweet = subset[-1:]\n",
    "            #RT parameters last 10 tweets\n",
    "            mean_RT_last10.append(np.mean(last10_tweets[\"retweet_count\"]))\n",
    "            sd_RT.append(np.std(last10_tweets[\"retweet_count\"]))\n",
    "            #FC parameters last 10 tweets\n",
    "            mean_FC_last10.append(np.mean(last10_tweets[\"favorite_count\"]))\n",
    "            sd_FC.append(np.std(last10_tweets[\"favorite_count\"]))\n",
    "            #take json user\n",
    "            j_user = subset[-1:][\"user\"].reset_index(drop=True)[0]\n",
    "            users.append(j_user)\n",
    "            #parameters of last tweet\n",
    "            y1.append(np.mean(last_tweet[\"favorite_count\"]))\n",
    "            y2.append(np.mean(last_tweet[\"retweet_count\"]))\n",
    "    output[\"j_user\"]=users\n",
    "    output[\"RT_l10\"]=mean_RT_last10\n",
    "    output[\"sd_RT\"]=sd_RT\n",
    "    output[\"FC_l10\"]=mean_FC_last10\n",
    "    output[\"sd_FC\"]=sd_FC\n",
    "    output[\"FC\"]=y1\n",
    "    output[\"RT\"]=y2\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert Prat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "output2 = author_dataset_conversion(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output2.to_csv(r\"C:/Users/Albert Prat/Documents/tweempact/datasets/author.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv(r\"C:/Users/Albert Prat/Documents/tweempact/datasets/tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trending_topic_df(df, path=\"c:/Users/Albert Prat/chromedriver.exe\", just_dates=False):    \n",
    "    from selenium import webdriver\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    \n",
    "    if just_dates==False:\n",
    "    # preparing elements to iterate\n",
    "        textdf = df[\"Tweet\"]\n",
    "        cadf = df[\"created_at\"]\n",
    "        cadf = [date[:10] for date in cadf]\n",
    "        cadf = np.unique(cadf)\n",
    "        cadf = [date.split(\"-\") for date in cadf]\n",
    "        iterable_for_adding_dummy = [date[2]+date[1]+date[0] for date in cadf]\n",
    "        # to be able to work with <, >\n",
    "    if just_dates == True:\n",
    "        iterable_for_adding_dummy = df\n",
    "    # setting the minimum date to look for and preparing the filters   \n",
    "    u = [datetime.datetime.strptime(date, \"%d%m%Y\") for date in iterable_for_adding_dummy]\n",
    "    threshold = datetime.datetime.strptime(\"01032015\", \"%d%m%Y\")\n",
    "    filtr = [u[i]>=threshold for i in range(len(u))]\n",
    "    filtr1 = [u[i]<threshold for i in range(len(u))]\n",
    "    \n",
    "    iterable = np.array(iterable_for_adding_dummy)[filtr]\n",
    "\n",
    "    # which of the unique dates is before threshold?\n",
    "    indices = [i for i, x in enumerate(filtr1) if x == True]\n",
    "    \n",
    "    # entering to browser and select the date box to be inputted\n",
    "    browser = webdriver.Chrome(path)\n",
    "    browser.get(\"https://trendogate.com/\")\n",
    "    date_dd = browser.find_element_by_xpath(\"//*[@id='userheaderbox']/div/form[2]/div[2]/input\")\n",
    "    \n",
    "    # preparing output lists\n",
    "    error = []\n",
    "    grepl = []\n",
    "    out = []\n",
    "    for i in range(len(iterable)):\n",
    "        browser.find_element_by_xpath(\"//select[@name='place']/option[text()='Spain']\").click()\n",
    "        date_dd = browser.find_element_by_xpath(\"//*[@id='userheaderbox']/div/form[2]/div[2]/input\")\n",
    "        if just_dates == False:\n",
    "            date = iterable[i][:10]\n",
    "        if just_dates == True:\n",
    "            date = iterable[i]\n",
    "        date_dd.send_keys(date)\n",
    "        browser.find_element_by_xpath(\"//*[@id='userheaderbox']/div/form[2]/button\").click()\n",
    "\n",
    "        if len(browser.find_elements_by_class_name(\"alert\")) == 1:\n",
    "            error.append(date)\n",
    "            out.append(None)\n",
    "            pass\n",
    "        else:\n",
    "            b = browser.find_element_by_class_name(\"col-lg-8\")\n",
    "            lgi = b.find_elements_by_class_name(\"list-group-item\")\n",
    "            TT = [el.text for el in lgi]\n",
    "            TT = \",\".join(TT)\n",
    "            out.append(TT)\n",
    "    for index in indices:\n",
    "        out.insert(index, None)\n",
    "    dates = [date[:2]+\"/\"+date[2:4]+\"/\"+date[4:] for date in iterable_for_adding_dummy]\n",
    "    tocsv = pd.DataFrame([dates, out]).T\n",
    "    tocsv.columns=[\"date\", \"trending_topics\"]\n",
    "    return tocsv\n",
    "def widen_TT_csv(df):\n",
    "    old = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\")\n",
    "    ca_old = old[\"created_at\"]\n",
    "    ca_old = [date[:10] for date in ca_old]\n",
    "    ca_old = np.unique(ca_old)\n",
    "    ca_old = [date.split(\"-\") for date in ca_old]\n",
    "    ca_old = [date[2]+date[1]+date[0] for date in ca_old]\n",
    "    \n",
    "    ca_new = df[\"created_at\"]\n",
    "    ca_new = [date[:10] for date in ca_new]\n",
    "    ca_new = np.unique(ca_new)\n",
    "    ca_new = [date.split(\"-\") for date in ca_new]\n",
    "    ca_new = [date[2]+date[1]+date[0] for date in ca_new]\n",
    "    \n",
    "    new_dates = set(ca_new) - set(ca_old)\n",
    "    \n",
    "    trend = trending_topic_df(list(new_dates), just_dates = True)\n",
    "    output = pd.concat([old, trend], axis = 0).reset_index(drop = True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttopics = trending_topic_df(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ttopics.to_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_dataset(df):\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    l = list(df[\"text\"])\n",
    "    filtr = [\"RT\" not in i for i in l]\n",
    "    df = df[filtr].reset_index(drop=True)\n",
    "    \n",
    "    Tweet =[re.sub(\"[(\\n)(\\r)]\",\" \",tweet) for tweet in df[\"text\"]]\n",
    "    entities = [tweet for tweet in df[\"entities\"]]\n",
    "    FC = [tweet for tweet in df[\"favorite_count\"]]\n",
    "    RT = [tweet for tweet in df[\"retweet_count\"]]\n",
    "    id_ = [tweet for tweet in df[\"id\"]]\n",
    "    j_user = [tweet for tweet in df[\"user\"]]\n",
    "    followers = [tweet[\"followers_count\"] for tweet in df[\"user\"]]\n",
    "    \n",
    "    CA = df[\"created_at\"]\n",
    "    CA_l = [time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(CA[i],'%a %b %d %H:%M:%S +0000 %Y'))for i in range(0,len(CA))]\n",
    "    \n",
    "    output = pd.DataFrame([id_,Tweet,j_user,CA_l,entities,followers, FC,RT]).T\n",
    "    output.reset_index(drop=True)\n",
    "    output.columns = [\"id\", \"Tweet\", \"j_user\",\"created_at\", \"entities\", \"followers_user\", \"FC\",\"RT\"]\n",
    "    \n",
    "    #trending topic df\n",
    "    #TT_new = widen_TT_csv(output)\n",
    "    #TT_new.to_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")\n",
    "    #TT_new = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "textdf = text_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32831, 8)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32831, 8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame.to_csv(output2, \"C:/Users/Albert Prat/Documents/tweempact/datasets/tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def widen_TT_csv(df):\n",
    "    old = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\")\n",
    "    ca_old = old[\"created_at\"]\n",
    "    ca_old = [date[:10] for date in ca_old]\n",
    "    ca_old = np.unique(ca_old)\n",
    "    ca_old = [date.split(\"-\") for date in ca_old]\n",
    "    ca_old = [date[2]+date[1]+date[0] for date in ca_old]\n",
    "    \n",
    "    ca_new = df[\"created_at\"]\n",
    "    ca_new = [date[:10] for date in ca_new]\n",
    "    ca_new = np.unique(ca_new)\n",
    "    ca_new = [date.split(\"-\") for date in ca_new]\n",
    "    ca_new = [date[2]+date[1]+date[0] for date in ca_new]\n",
    "    \n",
    "    new_dates = set(ca_new) - set(ca_old)\n",
    "    print(new_dates)\n",
    "    trend = trending_topic_df(list(new_dates), just_dates = True)\n",
    "    output = pd.concat([old, trend], axis = 0).reset_index(drop = True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "trending_topics = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def append_dummy_TT(df):\n",
    "    import re\n",
    "    import unidecode\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    \n",
    "    textdf = df[\"Tweet\"]\n",
    "    trending_topics = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")\n",
    "    TT = trending_topics[\"trending_topics\"]\n",
    "    indices = TT[TT.isnull()].index\n",
    "    # get TT as a list of lists\n",
    "    out = []\n",
    "    for i in range(len(TT)):\n",
    "        if i in indices:\n",
    "            out.append(None)\n",
    "        else:\n",
    "            t = TT[i].split(\",\")\n",
    "            out.append(t)\n",
    "    # date management\n",
    "    ca = list(df[\"created_at\"])\n",
    "    ca = [date[:10] for date in ca]\n",
    "    sp_d = [date.split(\"-\") for date in ca]\n",
    "    sp_d = [date[2]+date[1]+date[0] for date in sp_d]\n",
    "    # just keeping unique dates\n",
    "    ca = list(trending_topics[\"date\"])\n",
    "    unique_date = np.unique(ca)\n",
    "    unique_date = [date.split(\"/\") for date in unique_date]\n",
    "    unique_date = [date[2]+date[1]+date[0] for date in unique_date]\n",
    "    # append sp_d for the sake of indexing\n",
    "    df[\"spanish_date\"]=sp_d\n",
    "    grepl = []\n",
    "    for i in range(len(sp_d)):\n",
    "        ls = []\n",
    "        date = df[df.index == i][\"spanish_date\"][i]\n",
    "        tweet = textdf[i].lower()\n",
    "        tweet = unidecode.unidecode(tweet)\n",
    "        index = np.where(trending_topics[\"date\"]==(sp_d[i][:2]+\"/\"+sp_d[i][2:4]+\"/\"+sp_d[i][4:]))\n",
    "        TT = out[index[0][0]]\n",
    "        if TT == None:\n",
    "            grepl.append(None)\n",
    "        else:\n",
    "            for word in TT:\n",
    "                word = word.lower()\n",
    "                word = unidecode.unidecode(word)\n",
    "                word1 = re.sub(\" \", \"\", word)\n",
    "                ls.append(bool(re.search(r\"\\b\"+str(word)+r\"\\b\"+r\"|\\b\"+str(word1)+r\"\\b\", tweet)))\n",
    "            grepl.append(np.sum(ls))\n",
    "    df = df.drop(\"spanish_date\", axis = 1)\n",
    "    df[\"trending_topic\"]=grepl\n",
    "    df.reset_index(drop = True)\n",
    "    return df\n",
    "    df10 = df\n",
    "    \n",
    "    df1 = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\", index_col=0)\n",
    "    dropped = []\n",
    "    for column in df.columns:\n",
    "        dtype = type(df[column][0])\n",
    "        if dtype == dict:\n",
    "            df[column] = df[column].astype(str)\n",
    "            dropped.append(column)\n",
    "    df = pd.concat([df1,df], axis = 0).drop_duplicates().reset_index(drop=True)\n",
    "    df.reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = append_dummy_TT(textdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   66,   509,   556,   563,   639,   640,   826,  1129,  1145,\n",
       "         1179,  1189,  1392,  1393,  1398,  1771,  1783,  1797,  1990,\n",
       "         2207,  2209,  2261,  2737,  2927,  2955,  3319,  3685,  4293,\n",
       "         4473,  4655,  4795,  4929,  5362,  6096,  6102,  6103,  6288,\n",
       "         6706,  6709,  6712,  7375,  7438,  7771,  8400,  8529,  8557,\n",
       "         8585,  8769,  9066,  9322,  9524,  9526,  9872,  9962,  9963,\n",
       "         9984, 10050, 10113, 10340, 10341, 10630, 10766, 10767, 10768,\n",
       "        10769, 10770, 10994, 11202, 11622, 12286, 12364, 12368, 12688,\n",
       "        12692, 12694, 12792, 12796, 12943, 13001, 13010, 13128, 13276,\n",
       "        13837, 13887, 14040, 14041, 14042, 14043, 14044, 14046, 14371,\n",
       "        14476, 14542, 14679, 15020, 15179, 15240, 15577, 15579, 15581,\n",
       "        15840, 16174, 16198, 16237, 16239, 17461, 17857, 17981, 17982,\n",
       "        18105, 18166, 18290, 18307, 18312, 18314, 18315, 18666, 18687,\n",
       "        18709, 19048, 19104, 19144, 19149, 19153, 19515, 19542, 19733,\n",
       "        19752, 19756, 19759, 19867, 20051, 20053, 20161, 20192, 20193,\n",
       "        20596, 20754, 21299, 21577, 21599, 22311, 22375, 22483, 22528,\n",
       "        22529, 22530, 22649, 22653, 22819, 22828, 22867, 22920, 22921,\n",
       "        22926, 23030, 23263, 23389, 23498, 23501, 23583, 23585, 23591,\n",
       "        23604, 23608, 23610, 23847, 23912, 24020, 24066, 24545, 24603,\n",
       "        24670, 24892, 24893, 24898, 24900, 24908, 24909, 24920, 25016,\n",
       "        25017, 25020, 25034, 25037, 25043, 25305, 25470, 25472, 25478,\n",
       "        25491, 25577, 25668, 25794, 25850, 25884, 25888, 25890, 25906,\n",
       "        25911, 26154, 26199, 26209, 26306, 26308, 26478, 26777, 26778,\n",
       "        26888, 26924, 26985, 27014, 27016, 27017, 27060, 27085, 27086,\n",
       "        27087, 27088, 27089, 27108, 27109, 27240, 27242, 27663, 27883,\n",
       "        28050, 28156, 28204, 28205, 28208, 28346, 29084, 29103, 29259,\n",
       "        29260, 29302, 29951, 29982, 30202, 30203, 30214, 30893, 31055,\n",
       "        31058, 31108, 31109, 31111, 31112, 31119, 31179, 32045, 32336,\n",
       "        32337, 32567], dtype=int64),)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(out2[\"trending_topic\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Orange le arrebata República Móvil al grupo Masmóvil. ¿Qué razones hay para que el operador naranja haya cerrado es… https://t.co/SZadMFXfMa'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2[out2.index == 31111][\"Tweet\"][31111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-11-08 17:37:06'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2[out2.index == 31111][\"created_at\"][31111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.to_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42216"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51791, 9)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51791, 9)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32831, 9)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32831, 9)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdf[\"trending_topic\"] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
