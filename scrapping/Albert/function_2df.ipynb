{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime,time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!!!\n"
     ]
    }
   ],
   "source": [
    "# Connect with mongodb\n",
    "\n",
    "try:\n",
    "    with open(\"credentials.txt\", 'r', encoding='utf-8') as f:\n",
    "        [name,password,url,dbname]=f.read().splitlines()\n",
    "        conn=pymongo.MongoClient(\"mongodb://{}:{}@{}/{}\".format(name,password,url,dbname))\n",
    "    print (\"Connected successfully!!!\")\n",
    "except pymongo.errors.ConnectionFailure as e:\n",
    "    print (\"Could not connect to MongoDB: %s\" % e) \n",
    "    \n",
    "\n",
    "    \n",
    "db = conn['tweempact']\n",
    "collection = db['tweets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert Prat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: collection_names is deprecated. Use list_collection_names instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['system.indexes', 'tweets_noRT', 'users_noRT', 'tweets', 'tweet_users']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mongo(db, collection, query={}, host='localhost', port=27017, username=None, password=None, no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into DataFrame \"\"\"\n",
    "    \n",
    "    # Make a query to the specific DB and Collection\n",
    "    cursor = db[collection].find(query)\n",
    "\n",
    "    # Expand the cursor and construct the DataFrame\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "\n",
    "    # Delete the _id\n",
    "    if no_id:\n",
    "        del df['_id']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = read_mongo(db, \"tweets\")\n",
    "df2 = read_mongo(db, \"tweets_noRT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10490 19119\n"
     ]
    }
   ],
   "source": [
    "print(len(df1), len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    " def author_dataset_conversion(df):   \n",
    "    import time\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    cleaned = df[[\"id\", \"created_at\",\"entities\", \"extended_entities\",\"favorite_count\",\"retweet_count\",\"source\",\"text\", \"truncated\",\"user\",\"quoted_status_id\"]]\n",
    "    ids = [user[\"id\"] for user in cleaned[\"user\"]]\n",
    "    cleaned[\"id_num\"] = ids\n",
    "\n",
    "    #preparing output variables\n",
    "    output = pd.DataFrame()\n",
    "    users = []\n",
    "    mean_RT_last10,mean_FC_last10 = [],[]\n",
    "    sd_RT,sd_FC  = [],[]\n",
    "    y1,y2=[],[]\n",
    "\n",
    "    for _id in np.unique(ids):\n",
    "        subset = cleaned[cleaned[\"id_num\"]==_id].reset_index(drop=True)\n",
    "\n",
    "        #filtering tweets from all tweets and RT\n",
    "        l = list(subset[\"text\"])\n",
    "        filtr = [\"RT\" not in i for i in l]\n",
    "        subset = subset[pd.Series(filtr)]\n",
    "        if len(subset)>=11:\n",
    "            reindexed = subset[\"created_at\"].reset_index(drop=True)\n",
    "            #convert tweet date into python date\n",
    "            subset[\"created_at_pydate\"] = [time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(reindexed[i],'%a %b %d %H:%M:%S +0000 %Y'))for i in range(0,len(reindexed))]\n",
    "            subset = subset.sort_values(by=\"created_at_pydate\").reset_index(drop=True)\n",
    "\n",
    "            last10_tweets = subset[-11:-1]\n",
    "            last_tweet = subset[-1:]\n",
    "            #RT parameters last 10 tweets\n",
    "            mean_RT_last10.append(np.mean(last10_tweets[\"retweet_count\"]))\n",
    "            sd_RT.append(np.std(last10_tweets[\"retweet_count\"]))\n",
    "            #FC parameters last 10 tweets\n",
    "            mean_FC_last10.append(np.mean(last10_tweets[\"favorite_count\"]))\n",
    "            sd_FC.append(np.std(last10_tweets[\"favorite_count\"]))\n",
    "            #take json user\n",
    "            j_user = subset[-1:][\"user\"].reset_index(drop=True)[0]\n",
    "            users.append(j_user)\n",
    "            #parameters of last tweet\n",
    "            y1.append(np.mean(last_tweet[\"favorite_count\"]))\n",
    "            y2.append(np.mean(last_tweet[\"retweet_count\"]))\n",
    "    output[\"j_user\"]=users\n",
    "    output[\"RT_l10\"]=mean_RT_last10\n",
    "    output[\"sd_RT\"]=sd_RT\n",
    "    output[\"FC_l10\"]=mean_FC_last10\n",
    "    output[\"sd_FC\"]=sd_FC\n",
    "    output[\"FC\"]=y1\n",
    "    output[\"RT\"]=y2\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert Prat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "output1 = author_dataset_conversion(df1)\n",
    "output2 = author_dataset_conversion(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186 1406\n"
     ]
    }
   ],
   "source": [
    "print(len(output1),len(output2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.concat([output1, output2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(output, \"C:/Users/Albert Prat/Documents/tweempact/datasets/author.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_dataset(df):\n",
    "    import re\n",
    "    import pandas as pd\n",
    "    l = list(df[\"text\"])\n",
    "    filtr = [\"RT\" not in i for i in l]\n",
    "    df = df[filtr].reset_index(drop=True)\n",
    "    \n",
    "    Tweet =[re.sub(\"[(\\n)(\\r)]\",\" \",tweet) for tweet in df[\"text\"]]\n",
    "    entities = [tweet for tweet in df[\"entities\"]]\n",
    "    FC = [tweet for tweet in df[\"favorite_count\"]]\n",
    "    RT = [tweet for tweet in df[\"retweet_count\"]]\n",
    "    id_ = [tweet for tweet in df[\"id\"]]\n",
    "    j_user = [tweet for tweet in df[\"user\"]]\n",
    "    followers = [tweet[\"followers_count\"] for tweet in df[\"user\"]]\n",
    "    \n",
    "    CA = df[\"created_at\"]\n",
    "    CA_l = [time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(CA[i],'%a %b %d %H:%M:%S +0000 %Y'))for i in range(0,len(CA))]\n",
    "    \n",
    "    output = pd.DataFrame([id_,Tweet,j_user,CA_l,entities,followers, FC,RT]).T\n",
    "    output.reset_index(drop=True)\n",
    "    output.columns = [\"id\", \"Tweet\", \"j_user\",\"created_at\", \"entities\", \"followers_user\", \"FC\",\"RT\"]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = text_dataset(df1)\n",
    "output2 = text_dataset(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame.to_csv(output2, \"C:/Users/Albert Prat/Documents/tweempact/datasets/tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def trending_topic_df(df, path=\"c:/Users/Albert Prat/chromedriver.exe\", just_dates=False):    \n",
    "    from selenium import webdriver\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import datetime\n",
    "    \n",
    "    if just_dates==False:\n",
    "    # preparing elements to iterate\n",
    "        textdf = df[\"Tweet\"]\n",
    "        cadf = df[\"created_at\"]\n",
    "        cadf = [date[:10] for date in cadf]\n",
    "        cadf = np.unique(cadf)\n",
    "        cadf = [date.split(\"-\") for date in cadf]\n",
    "        iterable_for_adding_dummy = [date[2]+date[1]+date[0] for date in cadf]\n",
    "        # to be able to work with <, >\n",
    "    if just_dates == True:\n",
    "        iterable_for_adding_dummy = df\n",
    "    # setting the minimum date to look for and preparing the filters   \n",
    "    u = [datetime.datetime.strptime(date, \"%d%m%Y\") for date in iterable_for_adding_dummy]\n",
    "    threshold = datetime.datetime.strptime(\"01032015\", \"%d%m%Y\")\n",
    "    filtr = [u[i]>=threshold for i in range(len(u))]\n",
    "    filtr1 = [u[i]<threshold for i in range(len(u))]\n",
    "    \n",
    "    iterable = np.array(iterable_for_adding_dummy)[filtr]\n",
    "\n",
    "    # which of the unique dates is before threshold?\n",
    "    indices = [i for i, x in enumerate(filtr1) if x == True]\n",
    "    \n",
    "    # entering to browser and select the date box to be inputted\n",
    "    browser = webdriver.Chrome(path)\n",
    "    browser.get(\"https://trendogate.com/\")\n",
    "    date_dd = browser.find_element_by_xpath(\"//*[@id='userheaderbox']/div/form[2]/div[2]/input\")\n",
    "    \n",
    "    # preparing output lists\n",
    "    error = []\n",
    "    grepl = []\n",
    "    out = []\n",
    "    for i in range(len(iterable)):\n",
    "        browser.find_element_by_xpath(\"//select[@name='place']/option[text()='Spain']\").click()\n",
    "        date_dd = browser.find_element_by_xpath(\"//*[@id='userheaderbox']/div/form[2]/div[2]/input\")\n",
    "        if just_dates == False:\n",
    "            date = iterable[i][:10]\n",
    "            date = date.split(\"-\")\n",
    "            date = date[2]+date[1]+date[0]\n",
    "        if just_dates == True:\n",
    "            date = iterable[i]\n",
    "        date_dd.send_keys(date)\n",
    "        browser.find_element_by_xpath(\"//*[@id='userheaderbox']/div/form[2]/button\").click()\n",
    "\n",
    "        if len(browser.find_elements_by_class_name(\"alert\")) == 1:\n",
    "            error.append(date)\n",
    "            out.append(None)\n",
    "            pass\n",
    "        else:\n",
    "            b = browser.find_element_by_class_name(\"col-lg-8\")\n",
    "            lgi = b.find_elements_by_class_name(\"list-group-item\")\n",
    "            TT = [el.text for el in lgi]\n",
    "            TT = \",\".join(TT)\n",
    "            out.append(TT)\n",
    "    for index in indices:\n",
    "        out.insert(index, None)\n",
    "    dates = [date[:2]+\"/\"+date[2:4]+\"/\"+date[4:] for date in iterable_for_adding_dummy]\n",
    "    tocsv = pd.DataFrame([dates, out]).T\n",
    "    tocsv.columns=[\"date\", \"trending_topics\"]\n",
    "    return tocsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "tocsv = trending_topic_df(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def widen_TT_csv(df):\n",
    "    old = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\")\n",
    "    ca_old = old[\"created_at\"]\n",
    "    ca_old = [date[:10] for date in ca_old]\n",
    "    ca_old = np.unique(ca_old)\n",
    "    ca_old = [date.split(\"-\") for date in ca_old]\n",
    "    ca_old = [date[2]+date[1]+date[0] for date in ca_old]\n",
    "    \n",
    "    ca_new = df[\"created_at\"]\n",
    "    ca_new = [date[:10] for date in ca_new]\n",
    "    ca_new = np.unique(ca_new)\n",
    "    ca_new = [date.split(\"-\") for date in ca_new]\n",
    "    ca_new = [date[2]+date[1]+date[0] for date in ca_new]\n",
    "    \n",
    "    new_dates = set(ca_new) - set(ca_old)\n",
    "    \n",
    "    tocsv = trending_topic_df(list(new_dates), just_dates = True)\n",
    "    return tocsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = widen_TT_csv(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Albert Prat\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=True'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass sort=False\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "output = pd.concat([old, trend], axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.drop(output.columns[0], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64),)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(trending_topics[\"date\"]==\"14/08/2013\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ca = list(trending_topics[\"date\"])\n",
    "    unique_date = [date.split(\"/\") for date in ca]\n",
    "    unique_date = [date[2]+date[1]+date[0] for date in unique_date]\n",
    "    unique_date = np.unique(unique_date)\n",
    "    dates = []\n",
    "    for date in unique_date:\n",
    "        date_ = date[:4]+\"/\"+date[4:6]+\"/\"+date[6:]\n",
    "        date_=date_.split(\"/\")\n",
    "        date_ = date_[2]+date_[1]+date_[0]\n",
    "        dates.append(date_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.where(trending_topics[\"date\"]==(sp_d[1][:2]+\"/\"+sp_d[1][2:4]+\"/\"+sp_d[1][4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "399"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aquesta gent corrupta del @PPopular sempre diu que la corrupció és cosa del passat, però el passat del PP sembla se… https://t.co/NGzsq4nHyy'"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2[\"Tweet\"][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = [\"SergiRoberto es un patata\", \"sergi roberto es un patata\", \"sergi és un patata\", \"hola roberto\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for s in string:\n",
    "    s = s.lower()\n",
    "    print(bool(re.search(r\"\\b\"+str(\"sergi roberto\")+r\"\\b\"+r\"|\\b\"+str(\"sergiroberto\")+r\"\\b\", s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\bsergi roberto\\\\b|\\\\bsergiroberto\\\\b'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"\\b\"+str(\"sergi roberto\")+r\"\\b\"+r\"|\\b\"+str(\"sergiroberto\")+r\"\\b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'C:\\\\Users\\\\Albert Prat\\\\Documents\\\\tweempact\\\\datasets\\\\trending_topics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-341-8162c42aac2b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 1745\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    134\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    135\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m                                      compression=None)\n\u001b[0m\u001b[0;32m    137\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompression\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'C:\\\\Users\\\\Albert Prat\\\\Documents\\\\tweempact\\\\datasets\\\\trending_topics.csv'"
     ]
    }
   ],
   "source": [
    "output.to_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "a\n",
      "<class 'str'>\n",
      "a\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "<class 'str'>\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "for dataframe in [df1,output2]:\n",
    "    for column in dataframe.columns:\n",
    "        dtype = type(dataframe[column][0])\n",
    "        if dtype == dict:\n",
    "            print(\"a\")\n",
    "        else:\n",
    "            print(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'id': 2664945430, 'id_str': '2664945430', 'name': 'Fútbol Club Jumilla', 'screen_name': 'FCJumilla', 'location': 'Jumilla, España', 'description': 'Perfil Oficial del Fútbol Club Jumilla.\\\\nSegunda División B / Grupo IV @RFEF', 'url': 'https://t.co/VYPxc3mFj6', 'entities': {'url': {'urls': [{'url': 'https://t.co/VYPxc3mFj6', 'expanded_url': 'http://www.futbolclubjumilla.es', 'display_url': 'futbolclubjumilla.es', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 7162, 'friends_count': 334, 'listed_count': 92, 'created_at': 'Wed Jul 02 19:09:45 +0000 2014', 'favourites_count': 2804, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 8466, 'lang': 'es', 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'C0DEED', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1056155073703800832/zOE33Uob_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1056155073703800832/zOE33Uob_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2664945430/1538918974', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}\""
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[\"j_user\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'Tweet', 'j_user', 'created_at', 'entities', 'followers_user',\n",
       "       'FC', 'RT', 'trending_topic'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def append_dummy_TT(df):\n",
    "    import re\n",
    "    import unidecode\n",
    "    import pandas as pd\n",
    "    import ast\n",
    "    \n",
    "    textdf = df[\"Tweet\"]\n",
    "    trending_topics = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\trending_topics.csv\")\n",
    "    TT = trending_topics[\"trending_topics\"]\n",
    "    indices = TT[TT.isnull()].index\n",
    "    # get TT as a list of lists\n",
    "    out = []\n",
    "    for i in range(len(TT)):\n",
    "        if i in indices:\n",
    "            out.append(None)\n",
    "        else:\n",
    "            t = TT[i].split(\",\")\n",
    "            out.append(t)\n",
    "    # date management\n",
    "    ca = list(df[\"created_at\"])\n",
    "    ca = [date[:10] for date in ca]\n",
    "    sp_d = [date.split(\"-\") for date in ca]\n",
    "    sp_d = [date[2]+date[1]+date[0] for date in sp_d]\n",
    "    # just keeping unique dates\n",
    "    ca = list(trending_topics[\"date\"])\n",
    "    unique_date = np.unique(ca)\n",
    "    unique_date = [date.split(\"/\") for date in unique_date]\n",
    "    unique_date = [date[2]+date[1]+date[0] for date in unique_date]\n",
    "    # append sp_d for the sake of indexing\n",
    "    df[\"spanish_date\"]=sp_d\n",
    "    grepl = []\n",
    "    for i in range(len(sp_d)):\n",
    "        ls = []\n",
    "        date = df[df.index == i][\"spanish_date\"][i]\n",
    "        tweet = textdf[i].lower()\n",
    "        tweet = unidecode.unidecode(tweet)\n",
    "        index = np.where(trending_topics[\"date\"]==(sp_d[i][:2]+\"/\"+sp_d[i][2:4]+\"/\"+sp_d[i][4:]))\n",
    "        TT = out[index[0][0]]\n",
    "        if TT == None:\n",
    "            grepl.append(None)\n",
    "        else:\n",
    "            for word in TT:\n",
    "                word = word.lower()\n",
    "                word = unidecode.unidecode(word)\n",
    "                word1 = re.sub(\" \", \"\", word)\n",
    "                ls.append(bool(re.search(r\"\\b\"+str(word)+r\"\\b\"+r\"|\\b\"+str(word1)+r\"\\b\", tweet)))\n",
    "            grepl.append(np.sum(ls))\n",
    "    df = df.drop(\"spanish_date\", axis = 1)\n",
    "    df[\"trending_topic\"]=grepl\n",
    "    df.reset_index(drop = True)\n",
    "    \n",
    "    df1 = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\", index_col=0)\n",
    "    dropped = []\n",
    "    for column in df.columns:\n",
    "        dtype = type(df[column][0])\n",
    "        if dtype == dict:\n",
    "            df[column] = df[column].astype(str)\n",
    "            dropped.append(column)\n",
    "    df = pd.concat([df1,df], axis = 0).drop_duplicates().reset_index(drop=True)\n",
    "    df.reset_index(drop = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = append_dummy_TT(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2.to_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.read_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18968"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(r\"C:\\Users\\Albert Prat\\Documents\\tweempact\\datasets\\tweet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
